
@inproceedings{dreossi_compositional_2017,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Compositional {Falsification} of {Cyber}-{Physical} {Systems} with {Machine} {Learning} {Components}},
	isbn = {978-3-319-57288-8},
	doi = {10.1007/978-3-319-57288-8_26},
	abstract = {Cyber-physical systems (CPS), such as automotive systems, are starting to include sophisticated machine learning (ML) components. Their correctness, therefore, depends on properties of the inner ML modules. While learning algorithms aim to generalize from examples, they are only as good as the examples provided, and recent efforts have shown that they can produce inconsistent output under small adversarial perturbations. This raises the question: can the output from learning components can lead to a failure of the entire CPS? In this work, we address this question by formulating it as a problem of falsifying signal temporal logic (STL) specifications for CPS with ML components. We propose a compositional falsification framework where a temporal logic falsifier and a machine learning analyzer cooperate with the aim of finding falsifying executions of the considered model. The efficacy of the proposed technique is shown on an automatic emergency braking system model with a perception component based on deep neural networks.},
	language = {en},
	booktitle = {{NASA} {Formal} {Methods}},
	publisher = {Springer International Publishing},
	author = {Dreossi, Tommaso and Donzé, Alexandre and Seshia, Sanjit A.},
	editor = {Barrett, Clark and Davies, Misty and Kahsai, Temesghen},
	year = {2017},
	keywords = {Cyber-physical systems, Falsification, Machine learning, Temporal logic},
	pages = {357--372},
	file = {Full Text PDF:/home/remi/Zotero/storage/WN6HQVNB/Dreossi et al. - 2017 - Compositional Falsification of Cyber-Physical Syst.pdf:application/pdf},
}

@article{bao_deep_2017,
	title = {A deep learning framework for financial time series using stacked autoencoders and long-short term memory},
	volume = {12},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0180944},
	doi = {10.1371/journal.pone.0180944},
	abstract = {The application of deep learning approaches to finance has received a great deal of attention from both investors and researchers. This study presents a novel deep learning framework where wavelet transforms (WT), stacked autoencoders (SAEs) and long-short term memory (LSTM) are combined for stock price forecasting. The SAEs for hierarchically extracted deep features is introduced into stock price forecasting for the first time. The deep learning framework comprises three stages. First, the stock price time series is decomposed by WT to eliminate noise. Second, SAEs is applied to generate deep high-level features for predicting the stock price. Third, high-level denoising features are fed into LSTM to forecast the next day’s closing price. Six market indices and their corresponding index futures are chosen to examine the performance of the proposed model. Results show that the proposed model outperforms other similar models in both predictive accuracy and profitability performance.},
	language = {en},
	number = {7},
	urldate = {2022-04-28},
	journal = {PLOS ONE},
	author = {Bao, Wei and Yue, Jun and Rao, Yulei},
	year = {2017},
	note = {Publisher: Public Library of Science},
	keywords = {Deep learning, Neural networks, Finance, Forecasting, Memory, Recurrent neural networks, Stock markets, Wavelet transforms},
	pages = {e0180944},
	file = {Full Text PDF:/home/remi/Zotero/storage/3699MCSV/Bao et al. - 2017 - A deep learning framework for financial time serie.pdf:application/pdf;Snapshot:/home/remi/Zotero/storage/ED99JZSP/article.html:text/html},
}

@misc{noauthor_how_nodate,
	title = {How {Amazon} {Web} {Services} uses formal methods},
	url = {https://www.amazon.science/publications/how-amazon-web-services-uses-formal-methods},
	language = {en},
	urldate = {2022-04-29},
	journal = {Amazon Science},
	file = {Snapshot:/home/remi/Zotero/storage/TK8WCDJX/how-amazon-web-services-uses-formal-methods.html:text/html},
}

@inproceedings{passmore_imandra_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {The {Imandra} {Automated} {Reasoning} {System} ({System} {Description})},
	isbn = {978-3-030-51054-1},
	doi = {10.1007/978-3-030-51054-1_30},
	abstract = {We describe Imandra, a modern computational logic theorem prover designed to bridge the gap between decision procedures such as SMT, semi-automatic inductive provers of the Boyer-Moore family like ACL2, and interactive proof assistants for typed higher-order logics. Imandra’s logic is computational, based on a pure subset of OCaml in which all functions are terminating, with restrictions on types and higher-order functions that allow conjectures to be translated into multi-sorted first-order logic with theories, including arithmetic and datatypes. Imandra has novel features supporting large-scale industrial applications, including a seamless integration of bounded and unbounded verification, first-class computable counterexamples, efficiently executable models and a cloud-native architecture supporting live multiuser collaboration. The core reasoning mechanisms of Imandra are (i) a semi-complete procedure for finding models of formulas in the logic mentioned above, centered around the lazy expansion of recursive functions, (ii) an inductive waterfall and simplifier which “lifts” many Boyer-Moore ideas to our typed higher-order setting. These mechanisms are tightly integrated and subject to many forms of user control.},
	language = {en},
	booktitle = {Automated {Reasoning}},
	publisher = {Springer International Publishing},
	author = {Passmore, Grant and Cruanes, Simon and Ignatovich, Denis and Aitken, Dave and Bray, Matt and Kagan, Elijah and Kanishev, Kostya and Maclean, Ewen and Mometto, Nicola},
	editor = {Peltier, Nicolas and Sofronie-Stokkermans, Viorica},
	year = {2020},
	pages = {464--471},
	file = {Springer Full Text PDF:/home/remi/Zotero/storage/CTUGV5SP/Passmore et al. - 2020 - The Imandra Automated Reasoning System (System Des.pdf:application/pdf},
}

@inproceedings{passmore_lessons_2021,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Some {Lessons} {Learned} in the {Industrialization} of {Formal} {Methods} for {Financial} {Algorithms}},
	isbn = {978-3-030-90870-6},
	doi = {10.1007/978-3-030-90870-6_39},
	language = {en},
	booktitle = {Formal {Methods}},
	publisher = {Springer International Publishing},
	author = {Passmore, Grant Olney},
	editor = {Huisman, Marieke and Păsăreanu, Corina and Zhan, Naijun},
	year = {2021},
	pages = {717--721},
}

@book{boyer_computational_1979,
	title = {A {Computational} {Logic}},
	publisher = {ACM Monograph Series. Academic Press, New York},
	author = {Boyer, Robert S. and Moore, J. Strother},
	year = {1979},
}

@inproceedings{katz_reluplex_2017,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Reluplex: {An} {Efficient} {SMT} {Solver} for {Verifying} {Deep} {Neural} {Networks}},
	volume = {10426},
	url = {https://doi.org/10.1007/978-3-319-63387-9\_5},
	doi = {10.1007/978-3-319-63387-9_5},
	booktitle = {Computer {Aided} {Verification} - 29th {International} {Conference}, {CAV} 2017, {Heidelberg}, {Germany}, {July} 24-28, 2017, {Proceedings}, {Part} {I}},
	publisher = {Springer},
	author = {Katz, Guy and Barrett, Clark W. and Dill, David L. and Julian, Kyle and Kochenderfer, Mykel J.},
	editor = {Majumdar, Rupak and Kuncak, Viktor},
	year = {2017},
	pages = {97--117},
}

@inproceedings{huang_safety_2017,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Safety {Verification} of {Deep} {Neural} {Networks}},
	volume = {10426},
	url = {https://doi.org/10.1007/978-3-319-63387-9\_1},
	doi = {10.1007/978-3-319-63387-9_1},
	booktitle = {Computer {Aided} {Verification} - 29th {International} {Conference}, {CAV} 2017, {Heidelberg}, {Germany}, {July} 24-28, 2017, {Proceedings}, {Part} {I}},
	publisher = {Springer},
	author = {Huang, Xiaowei and Kwiatkowska, Marta and Wang, Sen and Wu, Min},
	editor = {Majumdar, Rupak and Kuncak, Viktor},
	year = {2017},
	pages = {3--29},
}

@inproceedings{carlini_towards_2017,
	address = {San Jose, CA, USA},
	title = {Towards {Evaluating} the {Robustness} of {Neural} {Networks}},
	isbn = {978-1-5090-5533-3},
	url = {http://ieeexplore.ieee.org/document/7958570/},
	doi = {10.1109/SP.2017.49},
	abstract = {Neural networks provide state-of-the-art results for most machine learning tasks. Unfortunately, neural networks are vulnerable to adversarial examples: given an input x and any target classiﬁcation t, it is possible to ﬁnd a new input x that is similar to x but classiﬁed as t. This makes it difﬁcult to apply neural networks in security-critical areas. Defensive distillation is a recently proposed approach that can take an arbitrary neural network, and increase its robustness, reducing the success rate of current attacks’ ability to ﬁnd adversarial examples from 95\% to 0.5\%.},
	language = {en},
	urldate = {2021-03-11},
	booktitle = {2017 {IEEE} {Symposium} on {Security} and {Privacy} ({SP})},
	publisher = {IEEE},
	author = {Carlini, Nicholas and Wagner, David},
	month = may,
	year = {2017},
	pages = {39--57},
	file = {Carlini et Wagner - 2017 - Towards Evaluating the Robustness of Neural Networ.pdf:/home/remi/Zotero/storage/YWXESI8T/Carlini et Wagner - 2017 - Towards Evaluating the Robustness of Neural Networ.pdf:application/pdf},
}

@article{singh_abstract_2019,
	title = {An abstract domain for certifying neural networks},
	volume = {3},
	issn = {2475-1421},
	url = {https://dl.acm.org/doi/10.1145/3290354},
	doi = {10.1145/3290354},
	abstract = {We present a novel method for scalable and precise certification of deep neural networks. The key technical insight behind our approach is a new abstract domain which combines floating point polyhedra with intervals and is equipped with abstract transformers specifically tailored to the setting of neural networks. Concretely, we introduce new transformers for affine transforms, the rectified linear unit (ReLU), sigmoid, tanh, and maxpool functions.
            We implemented our method in a system called DeepPoly and evaluated it extensively on a range of datasets, neural architectures (including defended networks), and specifications. Our experimental results indicate that DeepPoly is more precise than prior work while scaling to large networks.
            We also show how to combine DeepPoly with a form of abstraction refinement based on trace partitioning. This enables us to prove, for the first time, the robustness of the network when the input image is subjected to complex perturbations such as rotations that employ linear interpolation.},
	language = {en},
	number = {POPL},
	urldate = {2021-03-25},
	journal = {Proceedings of the ACM on Programming Languages},
	author = {Singh, Gagandeep and Gehr, Timon and Püschel, Markus and Vechev, Martin},
	month = jan,
	year = {2019},
	pages = {1--30},
	file = {Singh et al. - 2019 - An abstract domain for certifying neural networks.pdf:/home/remi/Zotero/storage/8CADEVEE/Singh et al. - 2019 - An abstract domain for certifying neural networks.pdf:application/pdf},
}
