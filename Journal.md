# 18 August 2023

One limitation of our work is that there is a discrepancy between the initial trained and verified model and the model encoded in the checked proofs: training and verification uses floating point numbers; Marabou uses overapproximation to mitigate the numerical instability, and rounds the values during the proof serialisation; the proof checker then uses exact real arithmetic to reason about the weights. Ultimately, though, if the checker validates a proof, it means that the encoded model satisfies the desired property and we can extract it from the proof for safe inference.

Arbitrary precision
# 17 August 2023

Meeting summary:
* LOPSTR paper: add the paragraph about precision in conclusion
* Tone down claims about entirely removing numerical instability in the intro
* add thanks to the reviewers

* For the proof checker verification:
	* double-check if possible to use naturals for `update_nth`
	* re-write `update_nth` so that there is no pattern matching
	* write lemmas about the basic functions (`zip`, `repeat`, `udpate_nth`)
* Do a high-level sketch of the proof plan

Frameworks for DNN training and inference, like PyTorch, typically use floating point numbers implementation of real numbers. 
The DNN verifier Marabou uses floating point numbers as well. In order to avoid numerical instability issues in comparisons, it uses approximation to the 10th decimal.
The Marabou proof is rounded to 10^-10. The proof checker in Imandra then uses exact real arithmetic to check the proof. As of now, the proof checker being exact does not mean that the proof of .

Would it be of value to try to "attack" a Marabou proof using Jia & Rinard's method, and then check the proof?

# 16 August 2023

Sorry for the lack of recent update, here are some:
* for the LOPSTR paper: we integrated the different comments. The only remaining thing is to integrate the discussion of precision at the different stages of the verification pipeline (see precision report below).
* Proof checker verification: I re-wrote the functions in `arithmetic.iml` (https://github.com/rdesmartin/imandra-marabou-proof-checking/blob/refactorisation/proof_checker/arithmetic.iml) to use `map` and `zip`

## Precision
* DNNs are trained and deployed with Python/C++ libraries (e.g. Tensorflow, PyTorch) using (by default) 32-bit floating point numbers (with support for mixed-precision floats, and 64-bit floats). ONNX uses the same data types as the traning/inference frameworks.
* DNN-verifiers implemented in C++ (like alpha, beta-Crown or Marabou) also use floating point numbers. In addition, Marabou uses a custom 10^-10 rounding -> effect on completeness/soundness?
* The JSON proof generated by Marabou uses the same rounding at 10^-10.
* The proof checker has arbitrary precision.

From there, our work has the following limitations: 
* the arbitrary precision real arithmetic library used is implicitly trusted
* we only provide arbitrary precision for the proof checker, there is still a discrepancy between the model and the verified version.

Possible solutions (from Jia & Rinard): 
* Use arbitrary precision DNN verifiers; not very practical one because arbitrary precision LP solvers exist but are not scalable
* DNN verifier uses over-approximate to mitigate floating point errors; this gives a "nearly-complete verifier"
* Use DNN quantisation/binarisation; this removes floating point numbers altogether.


# 9 August 2023

So right now we are trying to prove the theorem that if the upper bound of some linear combination is null, then there indeed exists no valid solution to the linear programming problem. 

In the paper, this corresponds to the theorem 1, in the UNSAT:

Let $A \in M_{m \times n} (\mathbb{R})$  and $l,x,u \in \mathbb{R}^n$, such that $A\cdot x = 0$ and $l \leq x \leq u$,	exactly one of these two options holds:
- The SAT case:
		$\exists x \in \mathbb{R}^n$ such that $A \cdot x = 0$ and $l \leq x \leq u$. 
-  The UNSAT case: $\exists w \in \mathbb{R}^m$ such
		that for all  $l \leq x \leq u$,  $w^\intercal \cdot A \cdot x < 0$, whereas $ 0 \cdot w = 0 $. Thus, $w$ is a proof of the constraints' unsatisfiability.
 	Moreover, these vectors can be constructed while executing the Simplex algorithm.

In our code, this is expressed like so, in the file https://github.com/rdesmartin/imandra-marabou-proof-checking/blob/refactorisation/verification/contradiction_verification.iml :

```ocaml
theorem contradiction_verification x contradiction tableau upper_bounds lower_bounds =
	is_bounded x upper_bounds lower_bounds &&
	check_contradiction contradiction tableau upper_bounds lower_bounds
	==>
	not (null_product tableau x)
;;
```

We proved some linear arithmetic properties on our , like "if xA = 0, any linear combination of 2 rows of A noted y, xy = 0":

```ocaml
lemma kernel_vector_for_any_row_linear_combination tableau x row1 c1 row2 c2 =

null_product tableau x &&

List.mem row1 tableau &&

List.mem row2 tableau &&

List.length x = List.length row1 &&

List.length x = List.length row2

==>

dot_product x (list_add (list_mult row1 c1) (list_mult row2 c2)) = 0.

[@@disable null_product, dot_product, List.mem, List.length, list_add, list_mult][@@apply kernel_vector_for_any_row tableau x row1, kernel_vector_for_any_row tableau x row2, zero_dot_product_for_linear_combination x row1 row2 c1 c2]

[@@auto]
```

In order to prove the `contradiction_verification` theorem, we then tried to generalise the previous lemma to any linear combination of any number of rows but got stuck.


We then tried to go top down, by defining some lemmas we thought were sufficient to prove `contradiction_verification` , accepting them as axioms, and applying them, but it didn't work either, I suspect because the lemmas we chose were not sufficient.

## Meeting 
* I mainly showed and got feedback on the refactoring using more idiomatic functional patterns (folds, maps, etc.) in the parts of the code that are called by `check_contradiction`. We decided that strategically this should be the main focus for now because the implementation will greatly influence the proof and refactoring can make it much easier.
* Use the previously linked textbook to help with such patterns in relation to theorem proving
* We managed to prove the main theorem `contradiction_verification` by accepting the [`dot_product_check_contradiction_eq`](https://github.com/rdesmartin/imandra-marabou-proof-checking/blob/ce429af0fec0205e36a080acff7a57c326687783/verification/contradiction_verification.iml#L213C10-L213C10) as an axiom (for some reason it hadn't worked when I had tried earlier). This is encouraging for a bottom-up approach to the proof and trying to prove `dot_product_check_contradiction_eq` as a next step.

# 4 August 2023 Meeting
- [ ] create separate version of code with only full lemmas: clearer and better for verification

In its current form, the proof checker is very "concrete", complex, close to the C++ implementation. Look into program logics, attempts at proving properties on concrete programs (e.g. in C).

Refactoring to have more abstract code would make it easier to reason about. -> Look at implementation of tableau algorithms in other proof assistants.

Think about what specification we want to prove: a) validity of the implementation or b) validity of the metatheory (Farkas Lemma)? How to specify a) without proving b)?

# 2 August 2023 Meeting

## LOPSTR Paper

To do: 
- [x] Move table to main text
- [ ] Address arbitrary precision in Imandra: what is proven, what do we trust (Zarith, Gnu MPAL)
- [ ] Check how precision is handled in Marabou and in ONNX
- [x] Ask Omri how precision was chosen for the serialisation
- [x] Implement Kathrin's comments
	- [x] typos
	- [ ] change reference to the extended version of the paper
- [ ] Thank reviewers
- [x] Double-check that `forall` -> `for any` still valid
- [x] Go back over reviewer's comments
	- [x] Explain colours in NN schema

Notes: the intro and conclusion are somewhat repetitive, ok for low stakes venue like LOPSTR but would need to be changed
Implement even small changes from the reviewer.

## Marabou Proof Checker Correctness Proof
Write in natural language what we are trying to prove. 

Describe the algorithm as code, add description as comments in the code for future reference.

Start list of issues with the code to discuss: 
- [ ] Error handling for dimensions mismatch: do we have to raise an error, or is it enough to add equal size precondition in the verification?

The selling point for CPP would be to have integration of DNN verification with ITP verification (like the car-wind example).

