\documentclass[]{article}

%opening
\title{Holistic Neural Network Verification with Imandra}
\author{Remi Desmartin}

\begin{document}

\maketitle

\begin{abstract}


The increased complexity of critical software systems has made it harder to reason about them and to predict edge failure cases.
A contributing factor to the complexification of critical systems is the introduction of artificial intelligence (AI) components, often relying on machine learning (ML) techniques.

This reason, among others, has led to an increased demand for automated verification tools. Imandra is one such tool, initially designed for the verification of FinTech systems like trading algorithms and blockchain smart contract infrastructure. Imandra is both a language, in which programs can be modeled and executed, and a reasoning engine.
The reasoning engine offers capabilities common to theorem provers such as SMT solving and induction reasoning, and original features (e.g. input space region decomposition); these allow to verify and describe programs' properties.

The integration of ML components adds a layer of complexity to the already nontrivial verification task for which Imandra was designed. Currently, verification of ML algorithms such as neural networks requires separate dedicated tools.

Thus the main goals of this thesis are to investigate:

\begin{enumerate}
\item the role of ML components in complex systems;

\item the methods of verification of these components

\item the integration of ML components' verification in larger verification projects

\item to produce a fully functional ML library in Imandra, which will include domain-specific proof heuristics.



	
\end{enumerate}

\end{abstract}

\iffalse

\subsection*{Summary}
I'd rather deploy a strategy that says: Here is Imandra and its various applications. These various applications may have ML/AI components in the future. This thesis will investigate (a) role of ML/AI components of complex systems (b) methods of verification of these (c) methods for integration of various AI/ML verification components into larger verification projects (d) The project will produce a fully functional AI/ML library in Imandra, which will include domain-specific proof heuristics.

\subsection*{Article Abstract} 

Neural networks are increasingly relied upon as components of complex
safety-critical systems such as autonomous vehicles. There is high demand for
tools and methods that embed neural network verification in a larger
verification cycle. However, neural network verification is problematic
due to a wide range of verification properties of interest,
each so far being amenable to verification only in specialised solvers.
The
properties one may want to verify range from ``adversarial'' robustness-to-noise
that is amenable to SMT-solving, to higher-order properties such as monotonicity
of a class of neural networks that may require inductive reasoning, and further
to domain-specific properties that lend themselves to the study of neural
network components and simulation of neural network execution.
Currently, there
are tools and languages addressing these aspects separately, but there is no
unified tool for undertaking all three.
In this paper we show how Imandra, a
functional programming language and a theorem prover originally designed for
verification, validation and simulation of FinTech systems can offer a holistic
infrastructure for neural network verification. To showcase Imandra's potential as a single code base,
we developed a novel library that
formalises convolutional neural networks in Imandra, and  covers different
important facets of neural network verification.



\textbf{Background.} Neural networks have been widely adopted in complex safety-critical systems, like autonomous cars or facial recognition software. Even though they are known to generalise well and perform well on problems with noisy data, neural networks are also sensitive to small perturbations in their input and they are vulnerable to outside attacks, called \emph{adversarial attacks}. These vulnerabilities undermine the public's trust in these AI systems and limit their applications.

Formal verification of neural networks can help ensure trust by proving that they have some desirable properties, like robustness, i.e. output's insensibility relative to input perturbation, or higher-level properties for a particular problem. 

Formal methods, that form the basis of formal verification, consist in applying mathematical methods to software verification in order to prove that a piece of software rigorously respects a set of specifications. Two main families of tools, SMT-based and approximation-based, scalability limitation, etc. \textit{to develop}

Currently, neural network verification is feasible with a variety of tools that address each apporach separately. These tools are seldom designed for non-specialists and lack a programmer-friendly interface, e.g. integration with a (general-purpose) programming language.

\textbf{Aims.} In this thesis, we will try to contribute to the programming language infrastructure for neural network verification. We want to demonstrate the integration of NNV in an industry-ready verification software with functional language in order to make it more accessible to non-verification specialists.

\textbf{Methods.} To that aim, we will use Imandra, a theorem prover with an embedded functional programming language. Preliminary experiments have shown that Imandra is capable of formalising both neural networks (including with complex architectures like CNN) and a variety of interesting properties (robustness and others).

\textbf{Results/Plan?} These preliminary experiments have shown that Imandra share some of the limitations of more specialised tools. However, our plan is to extend Imandra and improve NN and properties' modeling in order to try and overcome these limitations.

\textbf{Conclusions.} \textit{To do}




\subsection*{Master's Thesis Abstract}
Neural networks have known fast development in the past years; this has lead to their
deployment in safety-critical applications like autonomous vehicles or aircraft collision
detection systems. Recent research has focused on developing tools to guarantee their
reliability.
Some verification tools are based on formal methods. Satisfiability Modulo Theory
(SMT) solvers are a type of theorem provers that can automatically verify predicates;
by expressing neural networks and their properties as predicates, the latter can be
proved. Other verification tools rely on abstraction and offer more scalability. Other
research focuses on programming languages to verify neural networks, like functional
languages or probabilistic programming. New generation reasoning engines, like Iman-
dra, combine many of these characteristics in one tool: integrated functional language,
theorem-proving capabilities, and offer new features like input space segmentation and
visualisation tools.
The purpose of this thesis is to evaluate to what extent this new generation of integrated
tools can be applied to neural network verification. Throughout this thesis, Imandra
is used to represent neural network models and to reason about their properties. The
thesis presents a library for representing convolutional neural networks (CNNs) in Iman-
dra. Software for importing existing models into Imandra is also developed. A special
Imandra module is dedicated to definitions of several known robustness properties from
verification literature. In addition to defining and using known verification properties,
we use Imandra to suggest a novel approach to the structural analysis of CNNs. We
systematically evaluate the proposed Imandra library for usability, scalability, accuracy
and applicability. This helps to make conclusions about the advantages and limits of
new generation reasoning tools for neural network verification.


\subsection*{Schlumberger project description}

Fast development of machine learning (ML) algorithms in the past years have made them ubiquitous; they are deployed in many artificial intelligence (AI) systems like autonomous vehicles or aircraft autopilot software. As they are deployed in such critical applications, it is crucial to be able to explain their decisions and to guarantee the absence of serious errors to ensure trust. Our project is at the intersection of two research topics that aim for this goal: explainable AI (XAI) and formal verification.

XAI consists in representing in a human-readable form part of an ML algorithm’s decision process so that end-users can understand and evaluate its reasoning; this improves trust in the AI’s decisions.

Formal verification consists in applying mathematical approaches to software verification to ensure that a software rigorously respects a set of specifications. Notably, recent research has focused on formal verification’s application to neural networks, a family of ML algorithms.

This project aims to contribute to the programming language infrastructure for machine learning verification. Its goal is to integrate formal verification and explainability in programming languages in a lightweight manner so that ML engineers can use formally verified building blocks in their software without needing formal verification expertise.

To that aim, we will study to what extent high-level, expressive languages with rich type systems allow to easily express verification properties and to integrate explainability.

This study will lead to the implementation of a NN verification integration in a yet-to-be-chosen programming language.

In the first stages of this project, we will continue our preliminary work with Imandra, a reasoning tool. Imandra offers formal verification capabilities via induction reasoning and SMT-solving and embeds a high-level functional programming language. We have shown that it allows verifying robustness properties and include explainability. We now want to explore the possibility of formally verifying the properties that emerge from these explanations.

As the focus of this project is the language infrastructure of NN verification and not the verification, we will use existing verification tools and explainability frameworks such as Marabout and SHAP.

We will use Schlumberger’s neural and Bayesian network-based applications as use cases to verify properties such as robustness and fairness and to provide explainability.
\fi

\end{document}
