% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.21 of 2022/01/12
%
\documentclass[runningheads]{llncs}
%
\usepackage[T1]{fontenc}
% T1 fonts will be used to generate the final print and online PDFs,
% so please use T1 fonts in your manuscript whenever possible.
% Other font encondings may result in incorrect characters.
%
\usepackage{graphicx}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following two lines
% to display URLs in blue roman font according to Springer's eBook style:
%\usepackage{color}
%\renewcommand\UrlFont{\color{blue}\rmfamily}
%
\usepackage{listings}

\usepackage[dvipsnames]{xcolor}
% Setup listing style
\lstdefinestyle{mystyle} {
	aboveskip=20pt,
	keywordstyle=\color{blue},
	commentstyle=\itshape\color{purple},
	stringstyle=\color{PineGreen},
	basicstyle=\ttfamily\footnotesize,
	breakatwhitespace=false,         
	breaklines=true,                 
	captionpos=b,       
	%columns=fixed,             
	keepspaces=true,
	showspaces=false,                
	showstringspaces=false,
	showtabs=false,                  
	tabsize=2
}
\lstset{style=mystyle}

\usepackage{array}
\newcolumntype{L}{>{\centering\arraybackslash}m{0.14\textwidth}}

\newcolumntype{K}{>{\centering\arraybackslash}m{0.2\textwidth}}


\usepackage{comment,cite}
\usepackage{subcaption} % For subcaptions on subfigures
\usepackage{wrapfig}

\usepackage{todonotes}
\newcommand{\knote}[1]{\todo[inline, color=blue!20]{#1}}
\newcommand{\rnote}[1]{\todo[inline, color=green!20]{#1}}


\begin{document}
%
\title{Neural Networks in Imandra: Matrix Representation as a Verification Choice\thanks{E.Komendantskaya acknowledges support of EPSRC grant EP/T026952/1.}}
%
%\titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{Remi Desmartin\inst{1} \and
Grant Passmore\inst{2}\and
Ekaterina Kommendentskaya\inst{1}}
%
\authorrunning{R. Desmartin et al.}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{Heriot-Watt University, Edinburgh, UK\and
Imandra Inc. Austin TX, USA
\email{lncs@springer.com}\\
\url{http://www.springer.com/gp/computer-science/lncs} \and
ABC Institute, Rupert-Karls-University Heidelberg, Heidelberg, Germany\\
\email{\{abc,lncs\}@uni-heidelberg.de}}
%
\maketitle              % typeset the header of the contribution
%
\begin{abstract}
The demand for formal verification tools for neural networks has increased as neural networks have been deployed in a growing number of safety-critical applications. Matrices are a data structure essential to formalising neural networks. Functional programming languages  encourage diverse approaches to matrix definitions. This feature has already been successfully exploited in different applications. The question we ask is whether, and how, these ideas can be applied in neural network verification.
  A functional programming language Imandra combines the syntax of a functional programming language and the power of an automated theorem prover.
Using these two key features of Imandra, we explore how different implementations of matrices can influence automation of neural network verification.

\keywords{Neural networks  \and Automated reasoning \and Formal verification \and Functional programming \and Imandra.}
\end{abstract}
%
%
%

\section{Motivation}

Neural network (NN) verification was pioneered by the
SMT-solving~\cite{KaBaDiJuKo17Reluplex,HuangKWW17} and an abstract
interpretation~\cite{SinghGPV19,GeMiDrTsChVe18,AEHW20} communities. However, recently claims have been made that functional programming, too, can be valuable in this domain. 
 There is a library~\cite{MariaBLFGRG22}
formalising small rational-valued neural networks in Coq. A more sizeable
formalisation called MLCert~\cite{BS19} imports neural networks from Python, treats
floating point numbers as bit vectors, and proves properties describing the
generalisation bounds for the neural networks.
%
An $F^*$ %and \emph{Liquid Haskell}
%
formalisation~\cite{KokkeKKAA20} uses $F^*$ reals and
refinement types for proving robustness of networks trained in Python.

There are several options for defining neural networks in functional programming, ranging from
defining neurons as record types~\cite{MariaBLFGRG22} to treating them as
functions with refinement types~\cite{KokkeKKAA20}. But we claim that two general considerations
should be key to any NN formalisation choice of formalisation. Firstly, we must define neural networks
as executable functions, because we want to take advantage of executing  them   in
the functional language of choice. Secondly, a generic approach
to layer definitions is needed, particularly when we implement complex neural network architectures, such as convolutional layers.

These two essential requirements dictate that neural networks are represented as matrices, and that a programmer makes choices about matrix formalisation.
This extended abstract will explain these choices, and the consequences they imply, from the verification point of view.
We use Imandra~\cite{PassmoreCIABKKM20} to make these points, because Imandra is a functional programming language with tight integration of automated proving.

Imandra has been
successful as a user-friendly and scalable tool in the FinTech
domain~\cite{Passmore21}. The secret of its success lies in combination of the
best features of functional languages and interactive and automated theorem
provers. Imandra's logic is based on a pure, higher-order subset of OCaml, and
functions written in Imandra are at the same time valid OCaml code that can be
executed, or \emph{``simulated''}. Imandra's mode of interactive proof
development is based on a typed, higher-order lifting of the \emph{Boyer-Moore
waterfall}~\cite{BM79} for automated induction, tightly integrated with novel
techniques for SMT modulo recursive functions.

This paper builds upon a recent development of a NN library in Imandra~\cite{DPKD22}, but
discusses specifically the matrix representation choices and their consequences.  


\section{Matrices in Neural Network Formalisation}
We will illusrate the functional approach to neural network formalisation and will introduce the syntax of the Imandra programming language~\cite{PassmoreCIABKKM20} by means of an example.
When we say we want to formalise neural networks as functions,
essentially, we aim to be able to define a NN using just a line of code:

\begin{lstlisting}[language=caml, label={lst:model}]
  let cnn input =
      layer_0 input >>= layer_1 >>= layer_2 >>= layer_3
\end{lstlisting}

where each \lstinline?layer_i? is defined in a modular fashion. 


 To see that a functional approach to neural networks does not necessarily imply generic nature of the code, 
let us consider an example. 
A \emph{perceptron}, also known as a \emph{linear classifier}, classifies a given input vector $X = (x_1, ..., x_m)$ into one of two classes $c_1$ or $c_2$ by computing a linear combination of the input vector with a vector of synaptic weights $(w_0, w_1, ..., w_m)$, in which $w_0$ is often called an \emph{intercept} or \emph{bias}: 
%\begin{equation}
	$ f(X) = 	\sum_{i=1}^{m}w_ix_i + w_0 $.
%\end{equation}
If the result is positive, it classifies the input as $c_1$ and if negative as $c_2$. It effectively divides the input space along a hyperplane defined by
$\sum_{i=1}^{m}w_ix_i + w_0 = 0$. 

%\begin{example}
%	Figure \ref{fig:linear_classifier} shows a 2-dimensional input space with 2 classes. In this example, the division hyperplane is the line defined by $y = ax + b$. The perceptron's only weight is a and its bias b.
%\end{example}

%\begin{figure}[ht]
%	\centering
%	\includegraphics[width=1.0\textwidth]{Figures/linear_classifier.png}
%	\rule{35em}{0.5pt}
%	\caption[Linear classifier]{The division line of a linear classifier.}
%	\label{fig:linear_classifier}
%\end{figure}

In most classification problems, classes are not linearly separated. To handle such problems, we can apply a non-linear function $a$ called an \textit{activation function} to the linear combination of weights and inputs. The resulting definition of a perceptron $f$ is:
\begin{equation}\label{eq:perceptron}
	f(X) = a\left(\sum_{i=1}^{m}w_ix_i + w_0\right)
      \end{equation}

     Let us start with a naive prototype of perceptron in Imandra.  The Iris data set is a ``Hello World'' example in data mining; it represents 3 kinds of Iris flowers using 4 selected features.
      In Imandra, inputs can be represented as a data type: % \lstinline{iris_input}

\begin{lstlisting}[language=caml]
type iris_input = {
  sepal_len: real;
  sepal_width: real;
  petal_len: real;
  petal_width: real;}
\end{lstlisting}

\begin{comment}
To process inputs of this type in the future, we can simply define

\begin{lstlisting}[language=caml]
let process_iris_input (x: iris_input) =
  let x0 = x.sepal_len in
  let x1 = x.sepal_width in
  let x2 = x.petal_len in
  let x3 = x.petal_width in
  (x1, x2, x3, x4)
\end{lstlisting}
\end{comment}

And we define a perceptron as a function:
%Finally, we assume we have an access to some trained perceptron that already has appropriate real-valued weights, and we get the following function, as expected:

\begin{lstlisting}[language=caml]
let layer_0 (w0, w1, w2, w3, w4) (x1, x2, x3, x4) =
  relu (w0 +. w1 *. x1 +. w2 *. x2 +. w3 *. x3 +. w4 *. x4)
\end{lstlisting}
where \lstinline{*.} and   \lstinline{+.} are \emph{times} and \emph{plus} defined on reals.  Note the use of the \lstinline{relu} activation function, which returns $0$ for all negative inputs and acts as the identity function otherwise.

Already in this simple example, one perceptron is not sufficient, as we must map its output to three classes. We use the usual machine learning literature trick and define a further layer of $3$ neurons, each representing one class. Each of these neurons is itself a perceptron, with one incoming weight and one bias. This gives us:

\begin{lstlisting}[language=caml]
let layer_1 (w1, b1, w2, b2, w3, b3) f1 =
  let o1 = w1 *. f1 +. b1 in
  let o2 = w2 *. f1 +. b2 in
  let o3 = w3 *. f1 +. b3 in
  (o1, o2, o3)

let process_iris_output (c0, c1, c2) =
  if (c0 >=. c1) && (c0 >=. c2) then "setosa"
  else if (c1 >=. c0) && (c1 >=. c2) then "versicolor"
  else "virginica"
\end{lstlisting}

\noindent The second function maps the output of the three neurons to the three specified classes. This post-processing stage often takes a
form of an \emph{argmax} or \emph{softmax} function, which we omit. % the definitions here for simplicity.

And thus the resulting function that defines our neural network model is:

%Suppose the given perceptron weights are\\  \lstinline{let weights_0 = (1.0023211, 1.1538234, -0.30127743, 0.9319558, 2.179688)} and\\
%\lstinline{let weights_1 = (-2.651993, 0.81521773, -0.83343804,  0.27192873, -0.27463955,  -1.21521)}, in which case this is our resulting function:

\begin{lstlisting}[language=caml]
let model input = process_iris_input input
        |> layer_0 weights_0 |> layer_1 weights_1 |>
                                        process_iris_output
\end{lstlisting}

%\textbf{Lessons Learnt.}
Although our naive formalisation has some features that we desired from the start, i.e.\ it defines a neural network as a composition of functions,
it is too inflexible to work with arbitrary compositions of layers.
%The above style of definitions would mean we will need to define all weights in all layers manually, and
In neural networks with hundreds of weights in every layer this manual approach will quickly become infeasible (as well as error prone).
So, let us generalise this attempt from the level of individual neurons to the level of matrix operations.

%\textbf{Neural Network Layers.}
%For cases when there are more classes than two, we may want to compose a perceptron with another layer, in which individual neurons represent individual classes, 
%\subsection{Multi-Layer perceptrons} \label{background_mlp}
%Fully connected feedforward NNs, also called
%and the resulting composition would be
The composition of many perceptrons is often called a \emph{multi-layer perceptron (MLP)}.
%are the best-known type of deep NNs.
An MLP consists of an input vector (also called input layer in the literature),
multiple hidden layers and an output layer, each layer 
%The hidden layers and the output layer are
made of perceptrons with weighted connections to the previous layers' outputs. %Each node is perceptron described in the previous section.
%Each layer's output is a vector of outputs of the individual perceptrons.
The weight and biases of all the neurons in a layer can be represented by two matrices denoted by $W$ and $B$. By adapting equation \ref{eq:perceptron} to this matrix notation, a layer's output $L$ can be defined as:
\begin{equation}
	L(X) = a(X \cdot W + B)
\end{equation}
where the operator $\cdot $ denotes the dot product between $X$ and each row of $W$, %i.e. the sum of the products of their corresponding elements,
$X$ is the layer's input and $a$ is the activation function shared by all nodes in a layer. %To simplify the notation, we will write $XW$ for $X*W$ in the remainder of the paper.
As the dot product multiplies pointwise all inputs by all weights, such layers are often called \emph{fully-connected}. %We will see in the next section more complex kinds of layers.

%A layer's input is the output of the previous layer.
By denoting $a_k, W_k, B_k$ --- the activation function, weights and biases of the $k$th layer respectively, an MLP $F$ with $L$ layers is traditionally defined as:
\begin{equation}
	F(X) = a_L[B_L + W_L (a_{L-1}(B_{L-1} + W_{L-1}(...(a_1(B_1+W_1\cdot X)))))]
      \end{equation}

      At this stage, we are firmly committed to using matrices and matrix operations. And we have two key choices:
\begin{enumerate}
\item to represent matrices as lists of lists (and take advantage of the inductive data type \lstinline{List}),
  \item define matrices as functions from pairs to matrix elements,
     \item  or take advantage of record types, and define matrices as records.  
\end{enumerate}

The first choice was taken in \cite{heras_incidence_2011} (in the context of dependent types in Coq), in ~\cite{KokkeKKAA20} (in the context of refinement types of F$^*$) and in \cite{grant_sparse_1996} (for sparse matrix encodings in Haskell).  The difference between the first and second approaches was
discussed in~\cite{wood_vectors_2019} (in Agda, but with no neural network application in mind).
The third method was taken in ~\cite{MariaBLFGRG22} using Coq (though records there were used to encode individual neurons).

In the next three sections, we will systematise these three approaches using the same formalism and the same language, in order to understand  the influence they make on neural network verification.


\section{Matrices as Lists of Lists}
We start with re-using  Imandra's  \lstinline{List} library. Lists are defined as inductive data structures.
\knote{Remi, please give the full definition in Imandra's syntax?}

%And Imandra holds a comprehensive library of list operations, which we re-use in the definitions below.
We start with defining victors as lists, and matrices as lists of vectors. 

\begin{lstlisting}[frame=none, language=caml]
type 'a vector = 'a list
type 'a matrix = 'a vector list 
\end{lstlisting}

It is possible to extend this formalisation by using dependent~\cite{heras_incidence_2011} or refinement~\cite{KokkeKKAA20} types to check the matrix size. But in Imandra this facility is not directly available, and we will need to use exceptions (monadic operations) to check the matrix sizes.

As there is no built-in type available for matrices equivalent to \lstinline{List} for vectors, the \lstinline{Matrix} module implements a number of functions for basic operations needed throughout the implementation. For instance, \lstinline{map2} takes as inputs a function $f$ and two matrices $A$ and $B$ of the same dimensions and outputs a new matrix $C$ where each element $c_{i,j}$ is the result of $f(a_{i, j}, b_{i, j})$:

\begin{lstlisting}[frame=none, language=caml]
let rec map2 (f: 'a -> 'b -> 'c) (x: 'a matrix) (y: 'b matrix) = match x with
	| [] -> (match y with
		| [] -> Ok []
		| y::ys  -> Error "map2: invalid list length.")
	| x::xs -> match y with 
		| [] -> Error "map2: invalid list length." 
		| y::ys -> let hd = map2 f x y in
		let tl = map2 f xs ys in
		lift2 cons hd tl
\end{lstlisting}

This implementation allows us to define other useful functions in a concise way. For instance, the dot-product of two matrices, or the $L_0$ distance between two matrices are defined as:

\begin{lstlisting}[frame=none, language=caml]
let dot_product (a:real matrix) (b:real matrix) =
	let c = map2 ( *. ) a b in
	map sum c
\end{lstlisting}

%\lstinline{column} and \lstinline{nth} help to manipulate functions within matrices by returning a single column of values or a single value with given coordinates. 



A fully connected layer is then defined as a function \lstinline{fc} that takes
as parameters an activation function, a 2-dimensional matrix of
layer's weights and an input vector:
\begin{lstlisting}[caption=Fully connected layer implementation, language=caml, label={lst:fully_connected}]
let activation f w i = (* activation func., weights, input *)
let linear_combination m1 m2 = if (length m1) <> (length m2)
    then Error "invalid dimensions" 
    else map sum (Vec.map2 ( *. ) m1 m2) in
let i' = 1.::i in (* prepend 1. for bias *)
let z = linear_combination w i' in
map f z
	
let rec fc f (weights:real matrix) (input:real vector) = match weights with
| [] -> Ok []
| w::ws -> lift2 cons (activation f w input) (fc f ws input)
	
\end{lstlisting} 

Note that each row of the weights matrix
represents the weights for one of the layer's nodes. The bias for each node is
the first value of the weights vector, and $1$ is prepended to the input vector
when computing the linear combination of weights and input to account for that.



  It is now easy to see that our desired modular approach to composing layers works as stated. We may define the layers using the syntax:
  \lstinline{let layer_i = fc a weights}, where \lstinline{i} stands for \lstinline{0,1,2,3}, and \lstinline{a} stands for any chosen activation function. 

  Although natural, this formalisation of layers and networks suffers from two problems.  Firstly, it lacks the matrix dimension checks that were readily provided  via refinement types in~\cite{KokkeKKAA20}. This is because Imandra is based on a computational fragment of HOL, and has no refinement or dependent types. To mitigate this, the library we present performs explicit dimension checking via a {\tt result} monad, which clatters the code and adds additional computational checks.
  Secondly, the matrix definition via the list datatypes makes verification of neural networks very inefficient.
  This general effect has been already reported in \cite{KokkeKKAA20}, but it may be instructive to look into the problem from the Imandra perspective.

   Robustness of neural networks~\cite{CKDKKAE22} is best amenable to proofs by arithmetic manipulation. This explains the interest of the SMT-solving community in the topic, which started with using Z3 directly~\cite{HuangKWW17}, and has resulted in highly efficient SMT solvers specialised on robustness proofs for neural networks~\cite{KaBaDiJuKo17Reluplex,KatzHIJLLSTWZDK19}.   Imandra's waterfall method~\cite{PassmoreCIABKKM20} defines a default flow for the proof search, which starts with unrolling inductive definitions, simplification and rewriting.
  As a result, proofs of neural network robustness or proofs as in the ACAS Xu challenge, which should not rely on the matrix size induction,
  stall in Imandra.
  %Instead of performing arithmetic operation in the SMT-solver style, they go Imandra takes a false track with analysis of the inductive structure of matrices.   
  %  unrolling-based~\cite{PassmoreCIABKKM20} proofs of robustness inefficient, as even
 % accessing matrix elements typically involves unfolding several layers of recursion.

  There is another mode of proofs available in Imandra:  \lstinline{blast}, a tactic for SAT-based symbolic execution modulo
 higher-order recursive functions. Blast is an internal custom SAT/SMT solver that can be called explicitely with the appropriate tactic.
 However, \lstinline{blast} currently does not support real arithmetic. This
 requires us to \emph{quantize} the neural networks we use (i.e.\ convert them to
 integer weights) and results in a \emph{quantised NN library}~\cite{DPKD22}.
 However, even with quantisation and the use of Blast, Imandra fails to scale to the Acas Xu challenge, let alone neural networks used in computer vision.

 This also does not come as a surprise: as~\cite{KaBaDiJuKo17Reluplex} points out, general-purpose SMT solvers do not scale to NN verification challenges.
%Indeed, when we use our quantised library on quantised ACAS Xu neural networks, \lstinline{blast} does not terminate. 
This is why, the algorithm \lstinline{reluplex} was introduced in \cite{KaBaDiJuKo17Reluplex} as an additional heuristic to SMT solver algorithms;
\lstinline{reluplex} has since given rise to a domain specific solver Marabou~\cite{KatzHIJLLSTWZDK19}.
Connecting Imandra to Marabou may be a promising future direction. 

 However, there is a silver lining of this method of matrix formalisation.  When we formulate verification properties that genuinely require induction, formalisation of matrices as lists does result in more natural, and easily automatable proofs. For example, De Maria et al.~\cite{MariaBLFGRG22} formalise in Coq \emph{``neuronal
archetypes''} for biological neurons. Each archetype is a specialised kind of
perceptron, in which additional functions are added to amplify or inhibit the
perceptron's outputs. It is out of scope of this paper to formalise the neuronal
archetypes in Imandra, but we take methodological insight
from~\cite{MariaBLFGRG22}. In particular, ~\cite{MariaBLFGRG22} shows that there
are natural higher-order properties that one may want to verify.

To make a direct comparison, modern neural network
verifiers~\cite{KaBaDiJuKo17Reluplex,SinghGPV19} deal with verification tasks of
the form ``given a trained neural network $f$, and a property $P_1$ on its
inputs, verify that a property $P_2$ holds for $f$'s outputs''. However, the
formalisation in~\cite{MariaBLFGRG22} considers properties of the form ``any
neural network $f$ that satisfies a property $Q_1$, also satisfies a property
$Q_2$.'' Unsurprisingly, the former kind of properties can be resolved by
simplification and arithmetic, whereas the latter kind requires
induction on the structure of $f$ (as well as possibly nested induction on
parameters of $Q_1$).

Another distinguishing consequence of this approach is that it is orthogonal to
the community competition for scaling proofs to large networks: usually the
property $Q_1$ does not restrict the size of neural networks, but rather points
to their structural properties. Thus, implicitly we quantify over neural
networks of any size.

To emulate a property \emph{\`a la} de Maria et al.,  in~\cite{DPKD22} we  defined a general network monotonicity property: \emph{any fully connected
network with positive weights is \emph{monotone}, in the sense that, given
increasing positive inputs, its outputs will also increase}. There has been some
interest in monotone networks in the literature~\cite{JS98,WehenkelL19}.
Our experiments in~\cite{DPKD22} show that Imandra can prove such properties by induction on the networks'
structure almost automatically (with the help of a handful of auxiliary lemmas). And the proofs easily go through for both quantised and real-valued neural networks.

\knote{Remi, please check it is true? Anything else needs to be said here?}



\section{Matrices as Functions}

We now return to the verification challenge of ACAS Xu, which we failed to conquer with inductive matrix representation of the last section.
This time we ask whether representing matrices as functions and leveraging Imandra's native proof heuristics can help.

With this in mind, we redefine matrices as functions from indices to values, which gives
constant-time (recursion-free) access to matrix elements:

%caption=Matrices as Functions,
\begin{lstlisting}[ language=caml]
type arg =
  | Rows
  | Cols
  | Value of int * int

type 'a t = arg -> 'a

let nth (m: 'a t) (i: int) (j: int) : 'a = m (Value (i,j))
\end{lstlisting}

Note the use of the \lstinline{arg} type, which treats a matrix as a function evaluating ``queries'' (e.g., ``how many rows does this matrix have?'' or ``what is the value at index $(i,j)$?''). This formalisation technique is used as Imandra's logic does not allow function values inside of algebraic datatypes. % such as records.
We thus recover some functionality given by refinement types in~\cite{KokkeKKAA20}.  

Furthermore, we can map over a matrix, map2 over a pair of matrices, transpose a
matrix, construct a diagonal matrix etc.\ without any recursion, since we work
point-wise on the elements. At the same time, we remove the need for error
tracking to ensure matrices are of the correct size: because our matrices are
total functions, they are defined everywhere (even outside of their stated
dimensions), and we can make the convention that all matrices we build are valid
and sparse by construction (with default 0 outside of their dimension bounds).

For full definitions of matrix operations and layers, the reader is referred to~\cite{DPKD22}, but we will give some definitions here, mainly to convey the general style (and simplicity!) of the code.  Working with the ACAS Xu networks, a script transforms the original ACAS Xu networks into sparse functional matrix representation.
For example, layer 5 of one of the networks we used is defined as follows:

\knote{define fc, or say how it is defined}

\begin{lstlisting}[language=caml]
let layer5 = fc relu (
  function
  | Rows -> 50
  | Cols -> 51
  | Value (i,j) -> Map.get (i,j) layer5_map)

let layer5_map =
  Map.add (0,0) (1) @@
  Map.add (0,10) (-1) @@
  Map.add (0,29) (-1) @@
  ...
  Map.const 0
\end{lstlisting}

\noindent The sparsity effect is achieved by quantising the networks, as quantisation has the effect of rounding small values to $0$.
For example, the original network we use here had $13000$ non-zero weights and its quantized version has only $4154$ non-zero weights,
which gives a reduction in number of network parameters by $\approx$68.05\%. 
We do this mainly in order to reduce the amount of computation Imandra needs to perform.
However, unlike the  \lstinline{[@@blast]} experiment, this code  generically works for any types of values, integer or real.

\knote{I understand this has changed now, Remi, can you correct respectively please?}

With this representation, we are able to repeat the results of Katz et al ~\cite{KaBaDiJuKo17Reluplex} on the quantised networks, which is a big
improvement compared to the previous section. It is especially impressive that it comes ``for free'' with simply changing the underling matrix representations.

%This section shows the capability of Imandra as an automated prover for domain-specific verification.
Several factors played a role in automating the proof. Firstly, Imandra being a
higher-order functional language opened the way for us to experiment with
alternative matrix representations in the first place. By using maps for the
large matrices, we eliminate all recursion (and large case-splits) except for
matrix folds (which now come in only via the dot product), which allowed Imandra
to expand the recursive matrix computations on demand. 
%This is a flexibility a purely SMT-solving approach lacks.
Secondly, we did not have to rely on a ``theory'' of integer or real arithmetic as SMT solvers do; and therefore encoding this verification challenge
for real-valued networks was as easy as it would be for integer-valued networks.
%\knote{explain better the point about the theory -- may become clear when auto is better described}
Finally, Imandra's native simplifier contributed to the success. It works on a DAG representation of terms and speculatively expands instances of recursive functions, only as they are (heuristically seen to be) needed.
Incremental congruence closure and simplex data structures are shared across DAG nodes, and symbolic execution results are memoised.
%Moreover, forward-chaining rules (such as those characterizing `relu`) are only applied on demand.
Informally speaking, Imandra works lazily expanding out the linear algebra as it is needed, and eagerly with sharing information over the DAG.
Contrast this approach with that of \lstinline{reluplex} which, informally, starts with the linear algebra fully expanded, and then works to derive laziness and sharing. 

% \knote{The text about matrices as functions, and Imandra's clever DAG method comes in here }

Although Imandra's simplifier-based automation above could give us results which
\lstinline{blast} could not deliver for the same network, it still did not scale
to the original non-quantised (dense) ACAS Xu network. Contrast this with
domain-specific verifiers such as Marabou which are able to scale (modulo
potential floating point imprecision) to the full ACAS Xu.
%, and indeed to
%networks an order of magnitude larger.
We are encouraged that the results of
this section were achieved without tuning Imandra's generic proof automation
strategies, and hopeful that the development of neural-network specific tactics %and the integration of domain-specific tools
will help Imandra
scale to such networks in the future.


\section{Matrices as Records}


\section{Related Work and Conclusions}
\subsection{using Sized Lists or Vectors}

Grant et al. (\cite{grant_sparse_1996}) proposes 4 sparse list-based matrix implementations. They use an array-as-trees representation which allows to optimise for sparse arrays (subtrees where all the leafs are 0 are replaced by a 0-leaf).

Binary trees and lists of row-fragments: binary tree array of sparse Vectors defined as \lstinline{[(Int, [Double])]}

A generalised envelope scheme: matrix is cut up in sections
A quadtree scheme: Triangular matrix is split up in 2 triangular and a rectangular one. 
A standard quadtree structure is used for the rectangular matrix.

A Two-copy list of row-segments scheme: list of row-segments and list of column-segments in order to iterate over columns easily. Con: 2x more space is used; can be used to improve the 2 first previous methods (quadtree already bidimensional)

Pros of sparse list-based matrix representation: optimised for sparse matrices. Optimised for the specific operation considered in the paper (solving of linear systems of equations using a Cholesky scheme)

%list = inefficient random-access to data
%array = large storing size for sparse array 


\subsection{Refined Types}
%
Coq/Mathcomp/SSReflex: the size of the matrix is defined as a refinement type and used to check that matrix have the appropriate size in multiplications
Heras et al \cite{heras_incidence_2011} discuss implementation; correctness is checked at compile-time.

This is also used in Starchild/Lazuli \cite{kokke_neural_2020}.


\subsection{Arrays}
Grant et al. \cite{grant_sparse_1996} mentions using Haskell mutable arrays which are implemented using monadic operations. They stress that using a mutable array allows for modifying the array in place (thus saving memory), but it introduces "extra programming difficulties"; i.e. the use of monads makes the code less clear (as is the case in \ref{subs:monads}). 

In our case, since IML is pure, there is no access to mutable arrays.

\subsection{Monadic Operations to Check Size} \label{subs:monads}
Allows to check for valid matrix sizes when no refinement types are available. Intuitive first representation.

Drawback: introduces a lot of pattern matching, so a lot of split cases which increases the size of the program to check exponentially

Drawback: no optimisation for sparse matrices

\subsection{Matrix as Functions}
Imandra implementation; matrices are defined as total functions mapping indices to values.
Theory of uninterpreted functions.

Woods \cite{wood_vectors_2019} discusses the benefits of implementing matrices as functions in Agda.

\subsection{Matrix as Maps}

%
% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
% References will then be sorted and formatted in the correct style.
%
\bibliographystyle{splncs04}
\bibliography{MatrixFormalisation}

\end{document}
